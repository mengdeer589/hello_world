uv pip install "sglang[all]

python -m sglang.launch_server 
--model-path meta-llama/Meta-Llama-3-8B-Instruct 
--mem-fraction-static 0.7 #KV 缓存池的内存使用 默认0.9
--tp 2 #张量并行卡数
--tokenizer-path #Token 产生器的路径。
--context-length 4096
--host
--port 
--max-running-requests  #正在运行的请求的最大数量。
--device cuda
--api-key ""
--disable-cuda-graph-padding  #启用后，每个 batch 使用最接近的、≥ 当前 batch size 的 graph，更高效。
--mem-fraction-static 0.9
86,151 / 512 ≈ 168 个并发 #你设置了 --max-running-requests 1000，但显存最多支持 ~86k tokens，假设平均请求 512 tokens

genai-bench benchmark --api-backend openai             --api-base "http://127.0.0.1:30000"             --api-key 111            --api-model-name "qwen3"             --model-tokenizer "/workspace/qwen3"             --task text-to-text             --max-time-per-run 15             --max-requests-per-run 300             --server-engine "SGLang" --traffic-scenario "D(4096,256)" --num-concurrency 5 --num-concurrency 20

python -m sglang.launch_server --model-path qwen3 --context-length 4096 --max-running-requests 1000 --device cuda --api-key "" --chunked-prefill-size 4096

TORCH_CUDA_ARCH_LIST="8.9"
--cuda-graph-max-bs
source .venv/bin/activate
调优脚本
python tun.py --model qwen3 --dtype fp8_w8a8 --tune -tp-size 4 
python tun.py --model qwen3 --dtype fp8_w8a8 --tune
uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126
pip install "ray[all]"
usage: launch_server.py [-h] --model-path MODEL_PATH [--tokenizer-path TOKENIZER_PATH] [--tokenizer-mode {auto,slow}] [--skip-tokenizer-init]
                        [--load-format {auto,pt,safetensors,npcache,dummy,sharded_state,gguf,bitsandbytes,layered,remote}]
                        [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG] [--trust-remote-code] [--context-length CONTEXT_LENGTH] [--is-embedding]
                        [--enable-multimodal] [--revision REVISION] [--model-impl MODEL_IMPL] [--host HOST] [--port PORT] [--skip-server-warmup] [--warmups WARMUPS]
                        [--nccl-port NCCL_PORT] [--dtype {auto,half,float16,bfloat16,float,float32}]
                        [--quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt,modelopt_fp4,petit_nvfp4,w8a8_int8,w8a8_fp8,moe_wna16,qoq,w4afp8,mxfp4}]
                        [--quantization-param-path QUANTIZATION_PARAM_PATH] [--kv-cache-dtype {auto,fp8_e5m2,fp8_e4m3}] [--mem-fraction-static MEM_FRACTION_STATIC]
                        [--max-running-requests MAX_RUNNING_REQUESTS] [--max-queued-requests MAX_QUEUED_REQUESTS] [--max-total-tokens MAX_TOTAL_TOKENS]
                        [--chunked-prefill-size CHUNKED_PREFILL_SIZE] [--max-prefill-tokens MAX_PREFILL_TOKENS] [--schedule-policy {lpm,random,fcfs,dfs-weight,lof}]
                        [--schedule-conservativeness SCHEDULE_CONSERVATIVENESS] [--page-size PAGE_SIZE] [--hybrid-kvcache-ratio [HYBRID_KVCACHE_RATIO]]
                        [--swa-full-tokens-ratio SWA_FULL_TOKENS_RATIO] [--disable-hybrid-swa-memory] [--device DEVICE] [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
                        [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE] [--max-micro-batch-size MAX_MICRO_BATCH_SIZE] [--stream-interval STREAM_INTERVAL]
                        [--stream-output] [--random-seed RANDOM_SEED] [--constrained-json-whitespace-pattern CONSTRAINED_JSON_WHITESPACE_PATTERN]
                        [--watchdog-timeout WATCHDOG_TIMEOUT] [--dist-timeout DIST_TIMEOUT] [--download-dir DOWNLOAD_DIR] [--base-gpu-id BASE_GPU_ID]
                        [--gpu-id-step GPU_ID_STEP] [--sleep-on-idle] [--log-level LOG_LEVEL] [--log-level-http LOG_LEVEL_HTTP] [--log-requests]
                        [--log-requests-level {0,1,2,3}] [--crash-dump-folder CRASH_DUMP_FOLDER] [--show-time-cost] [--enable-metrics]
                        [--enable-metrics-for-all-schedulers] [--bucket-time-to-first-token BUCKET_TIME_TO_FIRST_TOKEN [BUCKET_TIME_TO_FIRST_TOKEN ...]]
                        [--bucket-inter-token-latency BUCKET_INTER_TOKEN_LATENCY [BUCKET_INTER_TOKEN_LATENCY ...]]
                        [--bucket-e2e-request-latency BUCKET_E2E_REQUEST_LATENCY [BUCKET_E2E_REQUEST_LATENCY ...]] [--collect-tokens-histogram]
                        [--gc-warning-threshold-secs GC_WARNING_THRESHOLD_SECS] [--decode-log-interval DECODE_LOG_INTERVAL] [--enable-request-time-stats-logging]
                        [--kv-events-config KV_EVENTS_CONFIG] [--api-key API_KEY] [--served-model-name SERVED_MODEL_NAME] [--weight-version WEIGHT_VERSION]
                        [--chat-template CHAT_TEMPLATE] [--completion-template COMPLETION_TEMPLATE] [--file-storage-path FILE_STORAGE_PATH] [--enable-cache-report]
                        [--reasoning-parser {deepseek-r1,deepseek-v3,glm45,gpt-oss,kimi,qwen3,qwen3-thinking,step3}]
                        [--tool-call-parser {llama3,qwen25,mistral,deepseekv3,deepseekv31,pythonic,kimi_k2,qwen3_coder,glm45,step3,gpt-oss}] [--tool-server TOOL_SERVER]
                        [--data-parallel-size DATA_PARALLEL_SIZE] [--load-balance-method {round_robin,shortest_queue,minimum_tokens}] [--dist-init-addr DIST_INIT_ADDR]
                        [--nnodes NNODES] [--node-rank NODE_RANK] [--json-model-override-args JSON_MODEL_OVERRIDE_ARGS]
                        [--preferred-sampling-params PREFERRED_SAMPLING_PARAMS] [--enable-lora] [--max-lora-rank MAX_LORA_RANK]
                        [--lora-target-modules [{q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj,qkv_proj,gate_up_proj,all} ...]] [--lora-paths [LORA_PATHS ...]]
                        [--max-loras-per-batch MAX_LORAS_PER_BATCH] [--max-loaded-loras MAX_LOADED_LORAS] [--lora-backend LORA_BACKEND]
                        [--attention-backend {triton,torch_native,cutlass_mla,fa3,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--prefill-attention-backend {triton,torch_native,cutlass_mla,fa3,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--decode-attention-backend {triton,torch_native,cutlass_mla,fa3,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--sampling-backend {flashinfer,pytorch}] [--grammar-backend {xgrammar,outlines,llguidance,none}] [--mm-attention-backend {sdpa,fa3,triton_attn}]
                        [--speculative-algorithm {EAGLE,EAGLE3,NEXTN}] [--speculative-draft-model-path SPECULATIVE_DRAFT_MODEL_PATH]
                        [--speculative-num-steps SPECULATIVE_NUM_STEPS] [--speculative-eagle-topk SPECULATIVE_EAGLE_TOPK]
                        [--speculative-num-draft-tokens SPECULATIVE_NUM_DRAFT_TOKENS] [--speculative-accept-threshold-single SPECULATIVE_ACCEPT_THRESHOLD_SINGLE]
                        [--speculative-accept-threshold-acc SPECULATIVE_ACCEPT_THRESHOLD_ACC] [--speculative-token-map SPECULATIVE_TOKEN_MAP]
                        [--expert-parallel-size EXPERT_PARALLEL_SIZE] [--moe-a2a-backend {none,deepep}]
                        [--moe-runner-backend {auto,triton,triton_kernel,flashinfer_trtllm,flashinfer_cutlass,flashinfer_mxfp4}]
                        [--flashinfer-mxfp4-moe-precision {mxfp4,bf16}] [--enable-flashinfer-allreduce-fusion] [--deepep-mode {normal,low_latency,auto}]
                        [--ep-num-redundant-experts EP_NUM_REDUNDANT_EXPERTS] [--ep-dispatch-algorithm EP_DISPATCH_ALGORITHM]
                        [--init-expert-location INIT_EXPERT_LOCATION] [--enable-eplb] [--eplb-algorithm EPLB_ALGORITHM]
                        [--eplb-rebalance-num-iterations EPLB_REBALANCE_NUM_ITERATIONS] [--eplb-rebalance-layers-per-chunk EPLB_REBALANCE_LAYERS_PER_CHUNK]
                        [--expert-distribution-recorder-mode EXPERT_DISTRIBUTION_RECORDER_MODE]
                        [--expert-distribution-recorder-buffer-size EXPERT_DISTRIBUTION_RECORDER_BUFFER_SIZE] [--enable-expert-distribution-metrics]
                        [--deepep-config DEEPEP_CONFIG] [--moe-dense-tp-size MOE_DENSE_TP_SIZE] [--enable-hierarchical-cache] [--hicache-ratio HICACHE_RATIO]
                        [--hicache-size HICACHE_SIZE] [--hicache-write-policy {write_back,write_through,write_through_selective}] [--hicache-io-backend {direct,kernel}]
                        [--hicache-mem-layout {layer_first,page_first}] [--hicache-storage-backend {file,mooncake,hf3fs,nixl}]
                        [--hicache-storage-prefetch-policy {best_effort,wait_complete,timeout}]
                        [--hicache-storage-backend-extra-config HICACHE_STORAGE_BACKEND_EXTRA_CONFIG] [--enable-double-sparsity]
                        [--ds-channel-config-path DS_CHANNEL_CONFIG_PATH] [--ds-heavy-channel-num DS_HEAVY_CHANNEL_NUM] [--ds-heavy-token-num DS_HEAVY_TOKEN_NUM]
                        [--ds-heavy-channel-type DS_HEAVY_CHANNEL_TYPE] [--ds-sparse-decode-threshold DS_SPARSE_DECODE_THRESHOLD] [--cpu-offload-gb CPU_OFFLOAD_GB]
                        [--offload-group-size OFFLOAD_GROUP_SIZE] [--offload-num-in-group OFFLOAD_NUM_IN_GROUP] [--offload-prefetch-step OFFLOAD_PREFETCH_STEP]
                        [--offload-mode OFFLOAD_MODE] [--disable-radix-cache] [--cuda-graph-max-bs CUDA_GRAPH_MAX_BS] [--cuda-graph-bs CUDA_GRAPH_BS [CUDA_GRAPH_BS ...]]
                        [--disable-cuda-graph] [--disable-cuda-graph-padding] [--enable-profile-cuda-graph] [--enable-cudagraph-gc] [--enable-nccl-nvls]
                        [--enable-symm-mem] [--disable-flashinfer-cutlass-moe-fp4-allgather] [--enable-tokenizer-batch-encode] [--disable-outlines-disk-cache]
                        [--disable-custom-all-reduce] [--enable-mscclpp] [--disable-overlap-schedule] [--enable-mixed-chunk] [--enable-dp-attention]
                        [--enable-dp-lm-head] [--enable-two-batch-overlap] [--tbo-token-distribution-threshold TBO_TOKEN_DISTRIBUTION_THRESHOLD] [--enable-torch-compile]
                        [--torch-compile-max-bs TORCH_COMPILE_MAX_BS] [--torchao-config TORCHAO_CONFIG] [--enable-nan-detection] [--enable-p2p-check]
                        [--triton-attention-reduce-in-fp32] [--triton-attention-num-kv-splits TRITON_ATTENTION_NUM_KV_SPLITS]
                        [--num-continuous-decode-steps NUM_CONTINUOUS_DECODE_STEPS] [--delete-ckpt-after-loading] [--enable-memory-saver] [--allow-auto-truncate]
                        [--enable-custom-logit-processor] [--flashinfer-mla-disable-ragged] [--disable-shared-experts-fusion] [--disable-chunked-prefix-cache]
                        [--disable-fast-image-processor] [--enable-return-hidden-states] [--scheduler-recv-interval SCHEDULER_RECV_INTERVAL]
                        [--debug-tensor-dump-output-folder DEBUG_TENSOR_DUMP_OUTPUT_FOLDER] [--debug-tensor-dump-input-file DEBUG_TENSOR_DUMP_INPUT_FILE]
                        [--debug-tensor-dump-inject DEBUG_TENSOR_DUMP_INJECT] [--debug-tensor-dump-prefill-only] [--disaggregation-mode {null,prefill,decode}]
                        [--disaggregation-transfer-backend {mooncake,nixl,ascend}] [--disaggregation-bootstrap-port DISAGGREGATION_BOOTSTRAP_PORT]
                        [--disaggregation-decode-tp DISAGGREGATION_DECODE_TP] [--disaggregation-decode-dp DISAGGREGATION_DECODE_DP]
                        [--disaggregation-prefill-pp DISAGGREGATION_PREFILL_PP] [--disaggregation-ib-device DISAGGREGATION_IB_DEVICE]
                        [--num-reserved-decode-tokens NUM_RESERVED_DECODE_TOKENS] [--pdlb-url PDLB_URL] [--custom-weight-loader [CUSTOM_WEIGHT_LOADER ...]]
                        [--weight-loader-disable-mmap] [--enable-pdmux] [--sm-group-num SM_GROUP_NUM] [--enable-ep-moe] [--enable-deepep-moe]
                        [--enable-flashinfer-cutlass-moe] [--enable-flashinfer-trtllm-moe] [--enable-triton-kernel-moe] [--enable-flashinfer-mxfp4-moe]